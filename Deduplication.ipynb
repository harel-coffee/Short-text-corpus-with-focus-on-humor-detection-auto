{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File succesfully imported. The file contains 1024 sentences.\n"
     ]
    }
   ],
   "source": [
    "#import the sentences that need processing\n",
    "import cPickle as pickle\n",
    "filename = 'proverbs'\n",
    "unprocessed_sentences= pickle.load(open(\"%s.pickle\"%filename))\n",
    "print \"File succesfully imported. The file contains %d sentences.\" %len(unprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 sentences\n",
      "processed 100 sentences\n",
      "processed 200 sentences\n",
      "processed 300 sentences\n",
      "processed 400 sentences\n",
      "processed 500 sentences\n",
      "processed 600 sentences\n",
      "processed 700 sentences\n",
      "processed 800 sentences\n",
      "processed 900 sentences\n",
      "processed 1000 sentences\n",
      "Removed items:\n",
      "[73, 591, 739, 805, 831]\n",
      "Amount of kept items:\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "#process loaded tweets\n",
    "import difflib\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import string\n",
    "import nltk.data\n",
    "import nltk.stem.snowball\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Get default English stopwords and extend with punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "\n",
    "def is_ci_token_stopword_set_match(a, b, threshold=0.9): \n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in nltk.word_tokenize(a)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in nltk.word_tokenize(b)\\\n",
    "                if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    # Calculate Jaccard similarity\n",
    "    ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return (ratio >= threshold)\n",
    "    \n",
    "# declare variables\n",
    "remove_items = []\n",
    "\n",
    "# Compare all sentences s for similarity with the sentences on index > s\n",
    "for i in range(len(unprocessed_sentences)+1):\n",
    "    for item in range(i+1,len(unprocessed_sentences)):\n",
    "        if is_ci_token_stopword_set_match(unprocessed_sentences[i], unprocessed_sentences[item]) == True:\n",
    "            remove_items.append(item)\n",
    "    if i%100 == 0:\n",
    "        print \"processed %d sentences\"%i\n",
    "\n",
    "        \n",
    "remove_items = list(OrderedDict.fromkeys(remove_items))\n",
    "print \"Removed items:\"\n",
    "print remove_items\n",
    "keep_sentences = []\n",
    "\n",
    "for item in range(len(unprocessed_sentences)):\n",
    "    if item not in remove_items:\n",
    "        keep_sentences.append(unprocessed_sentences[item])\n",
    "\n",
    "print \"Amount of kept items:\"\n",
    "print len(keep_sentences)\n",
    "\n",
    "#print len(unprocessed_sentences)   \n",
    "#print len(processed_sentences)\n",
    "\n",
    "# Write away processed data\n",
    "\n",
    "f = open(\"%s_deduplicated.pickle\"%filename, \"wb\")\n",
    "pickle.dump(keep_sentences, f)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
